{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7e6e1c-3c1a-4265-9bbe-7ab068b7007a",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad07667-2213-42ed-abff-6648a648c6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CocoCaptions\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "image_path = \"coco/images/val2017\"\n",
    "annotations = \"coco/annotations/captions_val2017.json\"\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "coco_dataset = CocoCaptions(root=image_path, annFile=annotations, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0fb635-a7d7-479d-83fd-9708cbd86df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a480914fc34725bcc2c10dd6d4dca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a369736443cc4cac9dc7ce85ab4ea12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dda75706df0491ba9d69d091d42bd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed56230a4b241e38d8a5c5a834cef6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468b932be13f4c5ba982767359eb5ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e62abb551546a7b36f2ed893c98e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976e83e2-ae98-44ce-b5be-723cb84354c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTokenizedDataset(Dataset):\n",
    "    def __init__(self, base_dataset, tokenizer, max_length=32):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, captions = self.base_dataset[idx]\n",
    "        caption = captions[0]\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokens.input_ids.squeeze(0)\n",
    "        attention_mask = tokens.attention_mask.squeeze(0)\n",
    "\n",
    "        return image, input_ids, attention_mask\n",
    "\n",
    "\n",
    "tokenized_coco = CocoTokenizedDataset(coco_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361a39d9-e4fd-471c-b299-ab7fa333c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_masks = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    return images, input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1020d661-f8c4-4feb-a766-2e2709dbfe6c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8bea25-e9b5-486c-9e21-13ba329d4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "# Image Encoder\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        base_model = resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # Output: (batch, 512, 1, 1)\n",
    "        self.fc = nn.Linear(512, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x).squeeze(-1).squeeze(-1)\n",
    "        embedding = self.fc(features)  # (batch, embed_dim)\n",
    "        embedding = F.normalize(embedding, dim=-1)  # L2-Norm\n",
    "        return embedding\n",
    "\n",
    "\n",
    "# Text Encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, max_len=32, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            key_padding_mask = ~attention_mask.bool()\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "\n",
    "        encoded = self.transformer_encoder(x, src_key_padding_mask=key_padding_mask) \n",
    "        encoded = encoded.permute(1, 0, 2)\n",
    "\n",
    "        pooled = encoded.mean(dim=1)\n",
    "\n",
    "        embedding = self.fc(pooled)\n",
    "        embedding = F.normalize(embedding, dim=-1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db93b12-799b-40a1-af38-40a374537042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIP(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, max_len=32):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(embed_dim)\n",
    "        self.text_encoder = TextEncoder(vocab_size, embed_dim, max_len)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        image_embeds = self.image_encoder(images)\n",
    "        text_embeds = self.text_encoder(input_ids, attention_mask)\n",
    "        return image_embeds, text_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6b160-f769-4f44-a3b7-f7d29f581da8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651fce27-bc09-4980-b085-a10091eb278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeds, text_embeds, temperature=0.07):\n",
    "    \"\"\"InfoNCE Loss\"\"\"\n",
    "    logits = torch.matmul(image_embeds, text_embeds.T)  # (batch, batch)\n",
    "    logits = logits / temperature\n",
    "\n",
    "    labels = torch.arange(len(image_embeds)).to(image_embeds.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)      # Image → Text\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)    # Text → Image\n",
    "\n",
    "    loss = (loss_i2t + loss_t2i) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f799a20-6537-442b-842e-c6852576619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(tokenized_coco, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e83aa3-4592-4a46-9367-bb7166c773b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb587b00-ad7d-44e1-bba7-546187975a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 157/157 [03:53<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 2.7697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 157/157 [04:01<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 1.3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 157/157 [03:52<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 0.4696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 157/157 [03:58<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 157/157 [03:59<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 0.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Model params\n",
    "embed_dim = 256\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_len = 32\n",
    "\n",
    "model = MiniCLIP(vocab_size=vocab_size, embed_dim=embed_dim, max_len=max_len).to(device)\n",
    "\n",
    "# Training params\n",
    "temperature = 0.07\n",
    "lr = 1e-4\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for images, input_ids, attention_masks in loop:\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "\n",
    "        image_embeds, text_embeds = model(images, input_ids, attention_masks)\n",
    "        loss = contrastive_loss(image_embeds, text_embeds, temperature)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3e1e9e2-09e9-49ee-8022-617ca47f2035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [03:41<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_dir = 'models/mini-clip'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "model.eval()\n",
    "all_img_embeds = []\n",
    "all_txt_embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, input_ids, attention_masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "\n",
    "        img_emb, txt_emb = model(images, input_ids, attention_masks)\n",
    "        all_img_embeds.append(img_emb.cpu())\n",
    "        all_txt_embeds.append(txt_emb.cpu())\n",
    "\n",
    "image_embeddings = torch.cat(all_img_embeds, dim=0)\n",
    "text_embeddings = torch.cat(all_txt_embeds, dim=0)\n",
    "\n",
    "torch.save(image_embeddings, f\"{save_dir}/image_embeds.pt\")\n",
    "torch.save(text_embeddings, f\"{save_dir}/text_embeds.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c616c1-ebea-4bfa-bc87-616ed3a2462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{save_dir}/mini_clip_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc842b64-da9d-4455-9073-5885d5ba5f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
